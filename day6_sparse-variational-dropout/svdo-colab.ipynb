{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse Variational Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://ars-ashuha.github.io/images/ss_vd1.png)\n",
    "![alt text](https://ars-ashuha.github.io/images/ss_vd2.png)\n",
    "\n",
    "- Variational Dropout Sparsifies Deep Neural Networks https://arxiv.org/abs/1701.05369\n",
    "- Cheating link https://github.com/ars-ashuha/sparse-vd-pytorch/blob/master/svdo-solution.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install http://download.pytorch.org/whl/cpu/torch-0.4.1-cp36-cp36m-linux_x86_64.whl \n",
    "!pip3 install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logger\n",
    "!pip install tabulate -q\n",
    "from google.colab import files\n",
    "src = list(files.upload().values())[0]\n",
    "open('logger.py','wb').write(src)\n",
    "from logger import Logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from logger import Logger\n",
    "from torch.nn import Parameter\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a dataset\n",
    "def get_mnist(batch_size):\n",
    "    trsnform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=True, download=True,\n",
    "        transform=trsnform), batch_size=batch_size, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=False, download=True,\n",
    "        transform=trsnform), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearSVDO(nn.Module):\n",
    "    def __init__(self, in_features, out_features, threshold, bias=True):\n",
    "        super(LinearSVDO, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.threshold = threshold\n",
    "\n",
    "        self.W = Parameter(torch.Tensor(out_features, in_features))\n",
    "        ###########################################################\n",
    "        ########         You Code should be here         ##########\n",
    "        # Create a Parameter to store log sigma\n",
    "        self.log_sigma = ...\n",
    "        ###########################################################\n",
    "        self.bias = Parameter(torch.Tensor(1, out_features))\n",
    "        \n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.bias.data.zero_()\n",
    "        self.W.data.normal_(0, 0.02)\n",
    "        self.log_sigma.data.fill_(-5)        \n",
    "        \n",
    "    def forward(self, x): \n",
    "        ###########################################################\n",
    "        ########         You Code should be here         ##########\n",
    "        if self.training:\n",
    "            lrt_mean = ... # Compute activation's mean e.g x.dot(W) + b\n",
    "            lrt_std = ...  # Compute activation's var e.g sqrt((x*x).dot(sigma * sigma) + 1e-8)\n",
    "            eps = ... # sample random noise\n",
    "            return lrt_mean + lrt_std * eps\n",
    "        \n",
    "        ########         If not training        ##########\n",
    "        self.log_alpha = ... # Evale log alpha as a function(log_sigma, W)\n",
    "        self.log_alpha = # Clip log alpha to be in [-10, 10] for numerical stability \n",
    "        W = ... # Prune out redundant wights e.g. W * mask(log_alpha < 3) \n",
    "        return F.linear(x, W) + self.bias\n",
    "        ###########################################################\n",
    "        \n",
    "    def kl_reg(self):\n",
    "        ###########################################################\n",
    "        ########         You Code should be here         ##########\n",
    "        ########  Eval Approximation of KL Divergence    ##########\n",
    "        # use torch.log1p for numerical stability\n",
    "        log_alpha = # Evale log alpha as a function(log_sigma, W)\n",
    "        log_alpha = # Clip log alpha to be in [-10, 10] for numerical suability \n",
    "        k1, k2, k3 = torch.Tensor([0.63576]), torch.Tensor([1.8732]), torch.Tensor([1.48695])\n",
    "        KL = ...\n",
    "        return KL \n",
    "        ########  Return a KL divergence, a Tensor 1x1   ##########\n",
    "        ###########################################################    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple 2 layer Network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, threshold):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = LinearSVDO(28*28, 300, threshold)\n",
    "        self.fc2 = LinearSVDO(300,  10, threshold)\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.log_softmax(self.fc2(x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new Loss Function -- SGVLB \n",
    "class SGVLB(nn.Module):\n",
    "    def __init__(self, net, train_size):\n",
    "        super(SGVLB, self).__init__()\n",
    "        self.train_size = train_size\n",
    "        self.net = net\n",
    "\n",
    "    def forward(self, input, target, kl_weight=1.0):\n",
    "        assert not target.requires_grad\n",
    "        kl = torch.Tensor([0.0])\n",
    "        for module in self.net.children():\n",
    "            if hasattr(module, 'kl_reg'):\n",
    "                kl = kl + module.kl_reg()\n",
    "        ###########################################################\n",
    "        ########         You Code should be here         ##########    \n",
    "        # Compute Stochastic Gradient Variational Lower Bound\n",
    "        # Do not forget to scale up Data term to N/M,\n",
    "        # where N is a size of the dataset and M is a size of minibatch\n",
    "        SGVLB = ...\n",
    "        return SGVLB # a Tensor 1x1 \n",
    "        ###########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(threshold=3)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[50,60,70,80], gamma=0.2)\n",
    "\n",
    "fmt = {'tr_los': '3.1e', 'te_loss': '3.1e', 'sp_0': '.3f', 'sp_1': '.3f', 'lr': '3.1e', 'kl': '.2f'}\n",
    "logger = Logger('sparse_vd', fmt=fmt)\n",
    "\n",
    "train_loader, test_loader = get_mnist(batch_size=100)\n",
    "sgvlb = SGVLB(model, len(train_loader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_weight = 0.02\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    scheduler.step()\n",
    "    model.train()\n",
    "    train_loss, train_acc = 0, 0 \n",
    "    kl_weight = min(kl_weight+0.02, 1)\n",
    "    logger.add_scalar(epoch, 'kl', kl_weight)\n",
    "    logger.add_scalar(epoch, 'lr', scheduler.get_lr()[0])\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data = data.view(-1, 28*28)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(data)\n",
    "        pred = output.data.max(1)[1] \n",
    "        loss = sgvlb(output, target, kl_weight)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss \n",
    "        train_acc += np.sum(pred.numpy() == target.data.numpy())\n",
    "\n",
    "    logger.add_scalar(epoch, 'tr_los', train_loss / len(train_loader.dataset))\n",
    "    logger.add_scalar(epoch, 'tr_acc', train_acc / len(train_loader.dataset) * 100)\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    test_loss, test_acc = 0, 0\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        data = data.view(-1, 28*28)\n",
    "        output = model(data)\n",
    "        test_loss += float(sgvlb(output, target, kl_weight))\n",
    "        pred = output.data.max(1)[1] \n",
    "        test_acc += np.sum(pred.numpy() == target.data.numpy())\n",
    "        \n",
    "    logger.add_scalar(epoch, 'te_loss', test_loss / len(test_loader.dataset))\n",
    "    logger.add_scalar(epoch, 'te_acc', test_acc / len(test_loader.dataset) * 100)\n",
    "    \n",
    "    for i, c in enumerate(model.children()):\n",
    "        if hasattr(c, 'kl_reg'):\n",
    "            logger.add_scalar(epoch, 'sp_%s' % i, (c.log_alpha.data.numpy() > model.threshold).mean())\n",
    "            \n",
    "    logger.iter_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_w, kep_w = 0, 0\n",
    "\n",
    "for c in model.children():\n",
    "    kep_w += (c.log_alpha.data.numpy() < model.threshold).sum()\n",
    "    all_w += c.log_alpha.data.numpy().size\n",
    "\n",
    "print('keept weight ratio =', all_w/kep_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # Good result should be like \n",
    "    #   epoch    kl       lr    tr_los    tr_acc    te_loss    te_acc    sp_0    sp_1\n",
    "    #  -------  ----  -------  --------  --------  ---------  --------  ------  ------\n",
    "    #      100     1  1.6e-06  -1.4e+03      98.0   -1.4e+03      98.3   0.969   0.760\n",
    "    # keept weight ratio = 30.109973454683352"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from matplotlib import rcParams\n",
    "rcParams['figure.figsize'] = 16, 3\n",
    "rcParams['figure.dpi'] = 300\n",
    "\n",
    "\n",
    "log_alpha = (model.fc1.log_alpha.detach().numpy() < 3).astype(np.float)\n",
    "W = model.fc1.W.detach().numpy()\n",
    "\n",
    "plt.imshow(log_alpha * W, cmap='hot', interpolation=None)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 0\n",
    "from matplotlib import rcParams\n",
    "rcParams['figure.figsize'] = 8, 5\n",
    "\n",
    "z = np.zeros((28*15, 28*15))\n",
    "\n",
    "for i in range(15):\n",
    "    for j in range(15):\n",
    "        s += 1\n",
    "        z[i*28:(i+1)*28, j*28:(j+1)*28] =  np.abs((log_alpha * W)[s].reshape(28, 28))\n",
    "        \n",
    "plt.imshow(z, cmap='hot_r')\n",
    "plt.colorbar()\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compression with Sparse Matrixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "from scipy.sparse import csc_matrix, csc_matrix, coo_matrix, dok_matrix\n",
    "\n",
    "row, col, data = [], [], []\n",
    "M = list(model.children())[0].W.data.numpy()\n",
    "LA = list(model.children())[0].log_alpha.data.numpy()\n",
    "\n",
    "for i in range(300):\n",
    "    for j in range(28*28):\n",
    "        if LA[i, j] < 3:\n",
    "            row += [i]\n",
    "            col += [j]\n",
    "            data += [M[i, j]]\n",
    "\n",
    "Mcsr = csc_matrix((data, (row, col)), shape=(300, 28*28))\n",
    "Mcsc = csc_matrix((data, (row, col)), shape=(300, 28*28))\n",
    "Mcoo = coo_matrix((data, (row, col)), shape=(300, 28*28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed('M_w', M)\n",
    "scipy.sparse.save_npz('Mcsr_w', Mcsr)\n",
    "scipy.sparse.save_npz('Mcsc_w', Mcsc)\n",
    "scipy.sparse.save_npz('Mcoo_w', Mcoo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls -lah | grep _w"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
